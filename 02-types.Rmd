# Data Types 

In every programming language, data is stored in different ways. Writing a program that manipulates data requires understanding all of the choices. That is why we must be concerned with the different **types** of data in our R and Python programs. Different types are suitable for different purposes.

There are similarities between Python's and R's type systems. However, there are may  differences as well. Be prepared for these differences. There are many more of them in this chapter than there were in the previous chapter!

If you're ever unsure what type a variable has, use `type()` (in Python) or `typeof()` (in R) to query it. 



```{r setup, include =FALSE}
library(reticulate)
use_condaenv("base")
```

<!---
---------------------------------------------------------------------------
-->


## Basic Types

Storing an individual piece of information is simple in both languages. However, while Python has scalar types, R's situation is a little more complicated.

### In Python

In Python, the simplest types we frequently use are `str` (short for string), `int` (short for integer), `float` (short for floating point) and `bool` (short for Boolean). This list is not exhaustive, but these are a good collection to start thinking about. For a complete list of built-in types in Python, click [here](https://docs.python.org/3/library/stdtypes.html).

```{python, collapse = TRUE}
print(type('a'), type(1), type(1.3))
```

Strings are useful for processing text data such as names of people/places/things and messages (e.g. texts, tweets and emails). If you are dealing with numbers, you need floating points if you have a number that might have a fractional part after its decimal; otherwise you'll need an integer. Booleans are useful for situations where you need to record whether something is true or false. They are also important to understand for control-flow in section \@ref(control-flow).

In the next section we will discuss the Numpy library. This library has a [broader collection](https://numpy.org/doc/stable/user/basics.types.html) of basic types. 

#### Type Conversions in Python

We will often have to convert between types in a Python program. This is called **type conversion**, and it can be either implicitly or explicitly done. 
For example, `int`s are often implicitly converted to `float`s, so that arithmetic operations work. 
```{python, collapse = TRUE}
my_int = 1
my_float = 3.2
my_sum = my_int + my_float
print("my_int's type", type(my_int))
print("my_float's type", type(my_float))
print(my_sum)
print("my_sum's type", type(my_sum))
```

You might be disappointed if you always count on this behavior, though.
```{python, error = TRUE, collapse=TRUE}
3.2 + "3.2"
```

Explicit conversions occur when we as programmers are explicitly asking Python to perform the conversion. You will do this with the functions `int()`, `str()`, `float()`, `bool()`, and the like. 

```{python, collapse = TRUE}
my_date = "5/2/2021"
month_day_year = my_date.split('/')
my_year = int(month_day_year[-1]) 
print('my_year is equal to ', my_year, 'and its type is ', type(my_year))
```

### In R

In R, the names of these basic types are only slightly different. They are `logical` (instead of `bool`), `integer` (instead of `int`), `double` (instead of `float`)^["double" is short for "double precision floating point." In the programming language `C`, which is what both R and Python are written in, the programmer has more control on how many decimal points of precision he wants.], and `character` (instead of `str`). There is also `complex` and `raw`, but we will use these less often in this textbook.

```{R, collapse = TRUE}
# cat() is kind of like print()
cat(typeof('a'), typeof(1), typeof(1.3))
```

In this case R automatically upgraded `1` to a double. If you wanted to force it to be an integer, you can add a capital "L" to the end of the number.

```{R, collapse = TRUE}
# cat() is kind of like print()
cat(typeof('a'), typeof(1L), typeof(1.3))
```


#### Type Conversions in R

You can explicitly and implicitly convert types in R just as you did in Python. Implicit conversion looks like this.

```{R, collapse = TRUE}
myInt = 1
myDouble = 3.2
mySum = myInt + myDouble
print(paste0("my_int's type is ", typeof(myInt)))
print(paste0("my_float's type is ", typeof(myDouble)))
print(mySum)
print(paste0("my_sum's type is ", typeof(mySum)))
```

Explicit conversion can be achieved with functions such as `as.integer`, `as.logical`, `as.double`, etc.

```{R, collapse = TRUE}
print(typeof(1))
print(typeof(as.logical(1)))
```



#### What was the weird thing about R you mentioned?

The basic types of R are a little different than the basic types of Python. R uses the same type to store many elements. It does not have any scalar type. On the other hand, Python has base types for individual elements, and it uses separate types as containers for storing many elements. In R, if you are looking at single number or character string, it's actually a length $1$ `vector`. More information about `vector`s can be found in section \@ref(r-vectors-versus-numpy-arrays-and-pandas-series).

TODO constants versus literals?


### Exercises

#### Easy

All answers to questions related to R should be written in a file named `easy_data_types_exercises.R`. All answers to questions related to Python should be written in a file named `easy_data_types_exercises.py`.


1. Which Python type is ideal for each piece of data? Assign your answers to a `list` of `str`ings called `question_one`. 

    + An individual's IP address
    + whether or not an individual attended a study
    + the number of seeds found in a plant
    + the amount of time it takes for a car to race around a track
  
2. Answer the same question above, but use R types? Assign your answers to a `character vector` of length four called `questionOne`. 

    + An individual's IP address
    + whether or not an individual attended a study
    + the number of seeds found in a plant
    + the amount of time it takes for a car to race around a track
  
3. ["The only numbers that can be represented exactly in R’s numeric type are integers and fractions whose denominator is a power of 2."](https://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f) Provide ten examples of numbers that have nonzero decimal components and are not exactly equal represented by R's `numeric` type. Store them all in a vector called `notExactFloats`.

<!---
---------------------------------------------------------------------------
-->


## R vectors versus Numpy `array`s (and Pandas `Series`)

This section is for describing the data types that let us store collections of elements that all **share the same type**. Data is very commonly stored in this fashion, so this section is quite important. Once we have one of these objects in a program, we will be interested in learning how to extract different subsets of elements, and how vectorization works.

### Overview of R

I mentioned earlier that R does not have scalar types--it just has [**vectors**](https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Vector-objects
). So, whether you want to store one number, or many numbers, you will need a `vector`. 

How do we create one of these? There are many ways. One common is to read in elements from an external data set, perhaps extracting them from a column of a `data.frame`, but we will describe those in section \@ref(data-frames). Here are some examples of generating `vector`s from code instead of grabbing them from an external data set after it is read in.

```{R, collapse = TRUE}
1:10
seq(1,10,2)
rep(2,5)
c("5/2/2021", "5/3/2021", "5/4/2021")
rnorm(10)
```

`c` is short for "combine". `seq` and `rep` are short for "sequence" and "replicate", respectively. `rnorm` samples normal (or Gaussian) random variables. There is plenty more to learn about these functions, so I encourage you to take a look at their documentation.

### Overview of Python

If you want to store many elements of the same type (and size) in Python, you will need a Numpy `array`. Numpy is a highly-regarded third party library [@harris2020array] for Python.

There are five ways to create numpy arrays ([source](https://numpy.org/doc/stable/user/basics.creation.html)). Here are some examples that complement the examples from above.

```{python, collapse = TRUE}
import numpy as np
np.array([1,2,3])
np.arange(1,12,2)
np.random.normal(size=3)
```

Another choice in Python is to use a [`Series` object](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas-series) from the `Pandas` library. The benefit of these is that they play nicely with Pandas data frames (more information about Pandas data frames can be found in \@ref(data-frames-in-python)), and that they have more flexibility with accessing elements by name ( see [here](https://jakevdp.github.io/PythonDataScienceHandbook/03.01-introducing-pandas-objects.html#Series-as-generalized-NumPy-array) for more information    ).

```{python, collapse = TRUE}
import pandas as pd
first = pd.Series([2, 4, 6])
second = pd.Series([2, 4, 6], index = ['a','b','c'])
print(first[0])
print(second['c'])
```


### Vectorization in R

An operation is **vectorized** if it applies to all of the elements of a `vector` at once. An operator that is not vectorized can only be applied to individual elements. In that case, the programmer would need to write more code to instruct the function to be applied to all of the elements of a vector. You should prefer writing vectorized code because it is easier to read, and quite often it runs much faster.

Arithmetic (e.g. `+`, `-`, `*`, `/`, `^`, `%%`, `%/%`, etc.) and logical (e.g. `!`, `|`, `&`, `>`, `>=`, `<`, `<=`, `==`, etc.) operators  are commonly applied on single vectors or between two vectors. Numeric vectors are converted to logical vectors if they need to be. Many functions work on vectors **element-wise** as well (except functions like `sum` or `length`, etc.). Operator precedence is important to understand if you seek to minimize your use of parentheses. Here are some examples.

```{r, collapse = TRUE}
(1:3) * (1:3)
(1:3) == rev(1:3)
sin( (2*pi/3)*(1:5))
```

In the last example, there is **recycling** happening. `(2*pi/3)` is taking three length-one vectors and producing another length-one vector. That gets multiplied by length five vector `1:5`. The single element in the length one vector gets recycled so that its value is multiplied by every element of `1:5`. 
This makes sense most of the time, but sometimes it can be tricky. Notice that this does not produce an error--just a warning.
```{r, collapse = TRUE}
(1:3) * (1:4)
```

### Vectorization in Python

The Python's Numpy library makes extensive use of vectorization as well. Vectorization in Numpy is accomplished with [**universal functions**](https://numpy.org/doc/stable/reference/ufuncs.html), or `ufunc` for short. Some `ufunc`s can be invoked using the same syntax as in R (e.g. `+`). You can also refer to function by name (e.g. `np.sum`). Mixing and matching is allowed, too.

`ufunc`s are called *unary* if they take in one array, and *binary* if they take in two. I reproduce a portion of the very nice table found in [@py_ds_handbook] 

| Operator |	Equivalent `ufunc` |
|-----------|-------------------|
`+`	| `np.add`	
`-`	| `np.subtract`	
`-`	| `np.negative`	
`*` |	`np.multiply`	
`/`	| `np.divide`	
`//` |	`np.floor_divide`	
`**`	| `np.power`	
`%` |	`np.mod`

For an exhaustive list of Numpy's universal functions, [click here.](https://numpy.org/doc/stable/reference/ufuncs.html#available-ufuncs) Here are some examples.


```{python, collapse=TRUE}
np.arange(1,4)*np.arange(1,4)
np.zeros(5) > np.arange(-3,2)
np.exp( -.5 * np.linspace(-3, 3, 10)**2) / np.sqrt( 2 * np.pi)
```

Instead of calling it "recycling", Numpy calls it [**broadcasting**](https://numpy.org/devdocs/user/theory.broadcasting.html). It's the same idea as in R, but in general, Python is stricter and disallows more scenarios. 

Then there are the `Series` objects from `Pandas`. ufuncs still work on `Series` objects, and they [respect common index values](https://jakevdp.github.io/PythonDataScienceHandbook/03.03-operations-in-pandas.html).

```{python, collapse = TRUE}
s1 = pd.Series(np.repeat(100,3))
s2 = pd.Series(np.repeat(10,3))
s1 + s2
```

If you feel more comfortable, and you want to coerce these `Series` objects to Numpy arrays, you can do that. For example, the following works. 

```{python, collapse = TRUE}
s = pd.Series(np.linspace(-1,1,5))
np.exp(s.to_numpy())
```

In addition, they possess many [attributes and methods](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas-series). 

```{python, collapse = TRUE}
ints = pd.Series(np.arange(10))
ints.abs()
ints.mean()
ints.floordiv(2)
```

`Series` objects that have [text data](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#working-with-text-data) are a little bit different. For one, you have to access the `.str` attribute of the `Series` before calling any [vectorized methods](https://jakevdp.github.io/PythonDataScienceHandbook/03.10-working-with-strings.html). Here is an example. 

```{python}
s = pd.Series(['a','b','c','33'])
s.dtype
s.str.isdigit()
s.str.replace('a', 'z')
```

String operations can be a big game changer. 

### Indexing `vector`s in R

It is very common to want to extract or modify a subset of elements in a vector. There are a few ways to do this. All of them involve the square bracket operator (e.g. `[]`). Feel free to retrieve the documentation by typing `?'['`.

```{R, collapse = TRUE}
allElements <- 1:6
allElements[seq(2,6,2)] # extract evens
allElements[-seq(2,6,2)] <- 99 # replace all odds with 99
allElements[allElements > 2] # get nums bigger than 20
```

To access the first element, we use the index `1`. To access the second, we use `2`, and so on. Also, the `-` sign tells R to remove elements. Both of these functionalities are *very different* from Python, as we will see shortly.

We can use names to access elements elements, too, but only if the elements are named.

```{R, collapse = TRUE}
sillyVec <- c("favorite"=1, "least favorite" = 2)
sillyVec['favorite']
```


### Indexing Numpy arrays

[Indexing Numpy arrays](https://numpy.org/doc/stable/user/basics.indexing.html) is very similar to indexing vectors in R. You use the square brackets, and you can do it with logical arrays or index arrays. There are some important differences, though. 

For one, indexing is 0-based in Python. The `0`th element is the first element of an array. Another key difference is that the `-` isn't used to remove elements like it is in R, but rather to count backwards. Third, using one or two `:` inside square brackets is more flexible in Python. This is syntactic sugar for using the `slice()` function, which is similar to R's `seq()` function. 

```{python, collapse=TRUE}
one_through_ten = np.arange(1, 11)
one_through_ten[np.array([2,3])]
one_through_ten[1:10:2] # evens
one_through_ten[::-1] # reversed
one_through_ten[-2] = 99 # second to last
one_through_ten
one_through_ten[one_through_ten > 3] # bigger than three
```

### Indexing Pandas Series

At a minimum, there is little that is new that you *need* to learn to go from Numpy arrays to Pandas Series objects. They still have the `[]` operator, and [many methods are shared across these two types](https://pandas.pydata.org/docs/reference/api/pandas.Series.html). The following is almost equivalent to the code above, and the only appparent difference is that the results are printed a little differently.

```{python, collapse=TRUE}
import pandas as pd
one_through_ten = pd.Series(np.arange(1, 11))
one_through_ten[np.array([2,3])]
one_through_ten[1:10:2] # evens
one_through_ten[::-1] # reversed
one_through_ten[-2] = 99 # second to last
one_through_ten
one_through_ten[one_through_ten > 3] # bigger than three
one_through_ten.sum()
```

However, [Pandas Series have `.loc` and `.iloc` methods](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#different-choices-for-indexing). We won't talk much about these two methods now, but they will become very important when we start to discuss Pandas Data Frames in section \@ref(data-frames-in-python).

```{python, collapse=TRUE}
one_through_ten.iloc[2]
one_through_ten.loc[2]
```

### Some Gotchas


#### Shallow versus Deep Copies

In R, assignment usually produces a **deep copy.** In the code below, we create `b` from `a`. If we modify `b`, these changes don't affect `a`. This takes up more memory, but our program is easier to follow as we don't have to keep track of connections between objects. 

```{r, collapse = TRUE}
# in R
a <- c(1,2,3)
b <- a
b[1] <- 999
a # still the same!
```

With Numpy arrays in Python, ["shallow copies" can be created by simple assignment, or by explicitly constructing a **view**](https://numpy.org/devdocs/user/quickstart.html#copies-and-views). In the code below, `a`, `b`, `c`, and `d` all share the same data. If you modify one, you change all the others. This can make the program more confusing, but on the other hand, it can also improve computational efficiency. 

```{python, collapse = TRUE}
# in python
a = np.array([1,2,3])
b = a # b is an alias
c = a.view() # c is a view
d = a[:]
b[0] = 999
a # two names for the same object in memory
b
c
d
```

It's the same story with Pandas Series objects. You're usually making a "shallow" copy.

```{python, collapse = TRUE}
# in python
import pandas as pd
s1 = pd.Series(np.array([100.0,200.0,300.0]))
s2 = s1
s3 = s1.view()
s4 = s1[:]
s1[0] = 999
s1
s2
s3
s4
```


If you want a "deep copy" in Python, you usually want a function or method called `copy()`. Use `np.copy` or [`np.ndarray.copy`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.copy.html#numpy-ndarray-copy) when you have a Numpy array.

```{python, collapse = TRUE}
# in python
a = np.array([1,2,3])
b = np.copy(a)
c = a.copy()
b[0] = 999
a 
b
c
```


Use [`pandas.Series.copy`](https://pandas.pydata.org/docs/reference/api/pandas.Series.copy.html#pandas-series-copy) with Pandas Series objects. Make sure not to set the `deep` argument to `False`. Otherwise you'll get a shallow copy.

```{python, collapse = TRUE}
# in python
s1 = pd.Series(np.array([1,2,3]))
s2 = s1.copy()
s3 = s1.copy(deep=False)
s1[0] = 999
s1
s2
s3
```

 
### How do R and Python handle missing values?

R has `NULL`,  `NaN`, and `NA`. Python has `None`, `np.nan`. If your eyes are glazing over already and you're thinking "they all look like the same"--they are not. 

R's `NULL` and Python's `None` are similar. Both represent "nothingness." This is *not* the same as `0`, or an empty string, or `FALSE`/`False`. This is commonly used to detect if a user fails to pass in an argument to a function, or if a function fails to "return" (more information on functions can be found in section \@ref(user-defined-functions-in-r-and-python)) anything meaningful. 

In R, for example, if a function fails to return anything, then it actually returns a `NULL`. 

```{r, collapse = TRUE, warning=TRUE}
NULL==FALSE
NULL==NULL
doNothingFunc <- function(a){}
thing <- doNothingFunc()
is.null(thing)
typeof(NULL)
```

In Python, we have the following.

```{python, collapse = TRUE}
None == False
None == None
def do_nothing_func():
  pass
thing = do_nothing_func()
if thing is None:
  print("thing is None!")
type(None)
```

"Nan" stands for "not a number." `NaN` is an object of type `double` in R, and `nan` is of type `float` in Python. It can come in handy when you have $0/0$ or $\infty / -\infty$.

```{r, collapse = TRUE}
# in R
0/0
Inf/Inf
is.na(0/0)
```
```{python, collapse = TRUE, error = TRUE}
# in Python
0/0
import numpy as np
np.inf/np.inf
np.isnan(np.nan)
```

"NA" is short for "not available." Missing data is a fact of life in data science. Observations are often missing in data sets, or introduced after joining/merging data sets together (more on this in section \@ref(merging-or-joining-data-sets)). There are many techniques designed to estimate quantities in the presence of missing data. When you code them up, you'll need to make sure you deal with `NA`s properly. 

```{r, collapse = TRUE}
# in R
babyData <- c(0,-1,9,NA,21)
NA == TRUE 
is.na(babyData)
typeof(NA)
```


Unfortunately, Python's support of an `NA`-like object is more limited. There is no `NA` object in base Python. And often `NaN`s will appear in place of an `NA`. There are a few useful tools, though. The Numpy library offers ["masked arrays"](https://numpy.org/devdocs/reference/maskedarray.html), for instance. 

Also, as of version `1.0.0`, the [Pandas library](https://pandas.pydata.org/docs/user_guide/index.html#user-guide) has an experimental `pd.NA` object. However, they [warn](https://pandas.pydata.org/pandas-docs/dev/user_guide/missing_data.html#missing-data-na) that "the behaviour of `pd.NA` can still change without warning." 

```{python, collapse = TRUE}
import numpy as np
import numpy.ma as ma
baby_data = ma.array([0,-1,9,-9999, 21]) # -9999 "stands for" missing
baby_data[3] = ma.masked
np.average(baby_data)
```

```{block, type='rmd-caution'}
Be careful of using extreme values to stand in for what should be an `NA`. Failing to mark the above missing value correctly would lead to extremely wrong calculations!
```



<!---
---------------------------------------------------------------------------
-->

## Numpy's `ndarray`s versus R's matrices and arrays

Sometimes you want a collection of elements that are all the same type, but you want to store them in a two- or three-dimensional structure. For instance, say you need to use matrix multiplication for some linear regression software you're writing, or that you needed to use tensors for a computer vision project you're working on. 

### In Python

In Python, you could still use arrays for these kinds of tasks. You will be pleased to learn that the Numpy `array`s we discussed earlier are a special case of [Numpy's N-dimensional arrays](https://numpy.org/doc/stable/reference/arrays.ndarray.html). Each array will come with an enormous amount of [methods](https://numpy.org/doc/stable/reference/arrays.ndarray.html#array-methods) and [attributes](https://numpy.org/doc/stable/reference/arrays.ndarray.html#array-attributes) (more on object-oriented program in chapter \@ref(an-introduction-to-object-oriented-programming)) attached to it. A few are demonstrated below. 

```{python, collapse = TRUE}
import numpy as np
a = np.array([[1,2],[3,4]], np.float)
a
a.shape
a.ndim
a.dtype
a.max()
a.resize((1,4)) # modification is **in place**
a
```


### In R

TODO [@matloff_r_book]

## R's lists versus Python's lists and dictionaries

When you need to store elements in a container, but you can't guarantee that these elements all have the same type, or you can't guarantee that they all have the same size, then you need a `list` in R. In Python, you might need a `list` or `dict` (short for dictionary). 

### Lists In R

`list`s are one of the most flexible data types in R. You can access individual elements in many different ways, each element can be of different size, and each element can be of a different type. 

```{r, collapse = TRUE}
myList <- list(c(1,2,3), "May 5th, 2021", c(TRUE, TRUE, FALSE))
myList[1] # length-1 list; first element is length 3 vector
myList[[1]] # length-3 vector
```

If you want to extract an element, you need to decide between using single square brackets or double square brackets. The former returns a `list`, while the second returns the type of the individual element.

You can also name the elements of a list. This can lead to more readable code. To see why, examine the example below. The `lm()` function estimates a linear regression model. It returns a `list` with plenty of components. 

```{r, collapse = TRUE, echo=TRUE, results='hide'}
dataSet <- read.csv("data/cars.csv")
results <- lm(log(Horsepower) ~ Type, data = dataSet)
length(results)
names(results)
results$contrasts
results['rank']
results[['terms']]
```


### Lists In Python

[Python `list`s](https://docs.python.org/3/library/stdtypes.html#lists) are very flexible, too. There are fewer choices for accessing elements of lists in Python--you'll most likely end up using the square bracket operator. Elements can be different sizes and types, just like they were with R's `list`s. 

Unlike in R, however, you cannot name elements of lists. If you want a container that allows you to access elements by name, look into Python [dictionaries](https://docs.python.org/3/library/stdtypes.html#mapping-types-dict) (see section \@ref(dictionaries-in-python)) or Pandas `Series` objects (see section \@ref(overview-of-python)).

From the example below, you can see that we've been introduced to lists already. We have been constructing Numpy arrays from them.

```{python, collapse = TRUE}
another_list = [np.array([1,2,3]), "May 5th, 2021", True, [42,42]]
another_list[2]
```

### Dictionaries In Python

[**Dictionaries**](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) in Python provide a container of key-value pairs. The keys are *unique*, and they must be *immutable*. `string`s are the most common key type, but `int`s can be used as well. 

Here is an example of creating a `dict` that stores the current price of a few popular cryptocurrencies. Accessing an individual element's value using its key is dont with the square bracket operator (i.e. `[]`), and deleting elements is done with the `del` keyword.

```{python, collapse = TRUE}
current_crypto_prices = {'BTC': 38657.14, 'ETH': 2386.54, 'DOGE': .308122}
current_crypto_prices['DOGE'] # get the current price of Dogecoin
del current_crypto_prices['BTC'] # remove the current price of Bitcoin
current_crypto_prices.keys()
current_crypto_prices.values()
```

You can also create `dict`s using **dictionary comprehensions** 

```{python, collapse= TRUE}
incr_cryptos = {key:val*1.1 for (key,val) in current_crypto_prices.items()}
incr_cryptos
```




## User-defined functions in R and Python

Why are functions important in statistical programming? In both R and Python (and every other programming language), functions are used to perform calculations. This is a pretty obvious statement.

This text has already covered how to *use* functions that come to us pre-made. At least we have discussed how to use them in a one-off way--just write the name of the function, write some parentheses after that name, and then plug in any requisite arguments by writing them in a comma-separated way between those two parentheses. This is how it works in both R and Python. 
In this section we take a look at how to define our own functions. This will help us understand pre-made functions. It will also be useful if we need some functionality that isn't already written for us. 

Writing our own functions is also useful for "packaging up" computations. The utility of this will become very apparent in chapter \@ref(functional-programming). Consider the task of estimating a regression model. Would you want to write that program using only arithmetic operators? Would it be simpler if you could use matrix multiplications? Would you want your function to work on different types of inputs? Would you want it to estimate several regression models and choose the "best" one?

Thankfully, R functions are very similar to Python functions. In both languages, functions are **first-class objects**. This means that, no matter which of these two languages you're using, functions

- can be passed as arguments to other functions, 
- they can be returned as values from other functions, and 
- they can be assigned to variables and stored in containers [@struc_and_interp]

### Defining R Functions

To create a function in R, we need another function called `function`. We give the output of `function` a name in the same way we give names to any other variable in R, by using the assignment operator `<-`. Here's an example of a toy function called `addOne`. Here `myInput` is a placeholder that refers to whatever the user of the function ends up plugging in. 

```{r, collapse = TRUE}
addOne <- function(myInput){  # define the function
  myOutput <- myInput + 1
  return(myOutput)
}
addOne(41) # call/invoke/use the function 
```

Below the definition, the function is called with an input of `41`. When this happens, the following sequence of events occurs

- The value `41` is assigned to `myInput`
- `myOutput` is given the value `42`
- `42` is returned from the function
- the temporary variables `myInput` and `myOutput` are destroyed. 

We get the desired answer, and all the unnecessary intermediate variables are cleaned up and thrown away. 


### Defining Python Functions

To create a function in Python, we use the `def` statement (instead of the `function` function in R). The desired name of the function comes next. After that, the formal parameters come, comma-separated inside parentheses, just like in R. 

Defining a function in Python is a little more concise. There is no assignment operator like there is in R, there are no curly braces, and `return` isn't a function like it is in R, so there is no need to use parentheses after it. There is one addition, though--we need a colon (`:`). 

Here is an example of a toy function called `add_one`.

```{python, collapse = TRUE}
def add_one(my_input):  # define the function
  my_output = my_input + 1
  return my_output
add_one(41) # call/invoke/use the function 
```

Below the definition, the function is called with an input of `41`. When this happens, the following sequence of events occurs

- The value `41` is assigned to `my_input`
- `my_output` is given the value `42`
- `42` is returned from the function
- the temporary variables `my_input` and `my_output` are destroyed. 

We get the desired answer, and all the unnecessary intermediate variables are cleaned up and thrown away. 

### More details on R's user-defined functions


Technically, in R, functions are [defined as three things bundled together](https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Function-objects): 

 1. a **formal argument list** (also known as *formals*), 
 2. a **body**, and 
 3. a **parent environment**.

The *formal argument list* is exactly what it sounds like. It is the list of arguments a function takes. You can access a function's formal argument list using the `formals()` function. Note that it is not the *actual* arguments a user will plug in--that isn't knowable at the time the function is created in the first place.

Here is another function that takes a *default* argument called `whichNumber`. If the user of the function doesn't specify how much she wants to add to `myInput`, `addNumber` will use `1` as the default. This default value shows up in the output of `formals(addNumber)`.

```{r, collapse = TRUE}
addNumber <- function(myInput, whichNumber = 1){  
  myOutput <- myInput + whichNumber
  return(myOutput)
}
addNumber(3) # no second argument being provided by the user here
formals(addNumber)
```

The function's *body* is also exactly what it sounds like. It is the work that a function performs. You can access a functions body using the the `body()` function. 

```{r, collapse = TRUE}
addNumber <- function(myInput, whichNumber = 1){  
  myOutput <- myInput + whichNumber
  return(myOutput)
}
body(addNumber)
```

Every function you create also has a  *parent environment*^[Primitive functions are functions that contain no R code and are internally implemented in C. These are the only type of function in R that don't have a parent environment.]. You can get/set this using the `environment()` function. Environments help a function know which variables it is allowed to use and how to use them. The parent environment of a function is where the function was *created*, and it contains variables outside of the body that the function can also use. The rules of which variables a function can use are called *scoping*. When you create functions in R, you are primarily using **lexical scoping**. To understand functions well in R, these examples are important to understand, so I provide more detail in \@ref(function-scope-in-r).

```{block, type='rmd-details'}
There is a lot more information about environments that isn't provided in this text. For instance, a user-defined function also has [binding, execution, and calling environments associated with it](http://adv-r.had.co.nz/Environments.html#function-envs), and environments are used in creating package namespaces, which are important when two packages each have a function with the same name.
```


### More details on Python's user-defined functions


Python functions have the same things R functions have: a **formal parameter list**, a body, and there are [namespaces](https://docs.python.org/3/tutorial/classes.html#python-scopes-and-namespaces) created that help organize which variables the function can access, as well as which pieces of code can call this new function. These three concepts are analogous to those in R. The names are just a bit different sometimes, and it isn't organized in the same way. 

Below is a table, taken straight from [the documentation](https://docs.python.org/3/reference/datamodel.html#objects-values-and-types), of all each user-defined function's *special attributes*. 

| Attribute |	Meaning |
|-----------|-------------------|
`__doc__`	| The function’s documentation string, or `None` if unavailable; not inherited by subclasses.
`__name__`	| The function’s name.	
`__qualname__`	| The function’s qualified name.	
`__module__` |	The name of the module the function was defined in, or None if unavailable.
`__defaults__`	| A tuple containing default argument values for those arguments that have defaults, or None if no arguments have a default value.
`__code__` |	The code object representing the compiled function body.
`__globals__`	| A reference to the dictionary that holds the function’s global variables — the global namespace of the module in which the function was defined.
`__dict__` |	The namespace supporting arbitrary function attributes.
`__closure__` | `None` or a tuple of cells that contain bindings for the function’s free variables. See below for information on the `cell_contents` attribute.
`__annotations__` | A dict containing annotations of parameters. The keys of the dict are the parameter names, and 'return' for the return annotation, if provided.
`__kwdefaults__` | A dict containing defaults for keyword-only parameters.

So take the  *formal parameter list* of a user-defined function, which is, again, the list of inputs a function takes. Just like in R, this is not the *actual* arguments a user will plug in--that isn't knowable at the time the function is created.^[You might have noticed that Python uses two different words to prevent confusion--unlike R, Python uses the word "parameter" (instead of "argument") to refer to the inputs a function takes, and "arguments" to the specific values a user plugs in.] Below we have another function that takes a **default argument** called `which_number`. If the user of the function doesn't specify how much she wants to add to `my_input`, `add_number` will use `1` as the default. This default value can be obtained with `add_number.__defaults__`.

```{python, collapse = TRUE}
def add_number(my_input, which_number = 1):
  my_output = my_input + which_number
  return my_output
add_number(3) # no second argument being provided by the user here
add_number.__code__.co_varnames # this also contains my_output
add_number.__defaults__
```
The code attribute has much more to offer. To see a list of names of all its contents, you can use `dir(add_number.__code__)`.

```{block, type='rmd-details'}
Don't worry if the notation `add_number.__code__` looks strange. The dot (`.`) operator will become more clear in the future chapter on *object-oriented programming*. For now, just think of `__code__` as being an object *belonging to* `add_number`. Objects that belong to other objects are called **attributes** in Python. The dot operator helps us access attributes *inside* other objects. 
```



### Function Scope in R

R uses **lexical scoping**.

R functions can use variables that are defined in the function body, and variables that were defined in the environment that the function itself was defined in. R functions **cannot** necessarily find variables in an environment where the function was *called* in. Code outside the body of a function cannot access variables inside the body of a function.

```{r, collapse = TRUE}
a <- 3
sillyFunction <- function(){
  return(a + 20) 
}
environment(sillyFunction) # the env. it was defined in contains a
sillyFunction()
```

From the point of view of the function, when it attempts to access a variable, it first looks in its own body. In the example below, there are two variables named `a`, but they exist in different environments. Inside the function, the innermost one gets used. Outside the function, the global variable gets used. 

```{r, collapse = TRUE}
a <- 3
sillyFunction <- function(){
  a <- 20
  return(a + 20) 
}
sillyFunction()
print(a)
```

The same concept applies if you create functions within functions. The inner function looks "inside-out" for variables. Below we call `outerFunc()`, which calls `innerFunc()`. `innerFunc()` can refer to the variable `b`, because it lies in the same environment in which `innerFunc()` was created. Interestingly, `innerFunc()` can also refer to the variable `a`, because that variable was captured by `outerFunc`, which provides access to `innerFunc`. 

```{r, collapse = TRUE}
a <- "outside both"
outerFunc <- function(){
  b <- "inside one"
  innerFunc <- function(){
    print(a) 
    print(b)
  }
  return(innerFunc())
}
outerFunc()
```

If we ask `outerFunc` to return the function `innerFunc` (functions are objects!), then we might be surprised to see that `innerFunc()` can still successfully refer to `b`, even though it doesn't exist inside the *calling environment.* But don't be surprised! What matters is what was available when the function was *created*. In this example, `outerFuncV2` is sometimes called a *function factory*. More information about this is provided in \@ref(functional-programming).

```{r, collapse = TRUE}
outerFuncV2 <- function(){
  b <- "inside one"
  innerFunc <- function(){
    print(b)
  }
  return(innerFunc) # note the missing inner parentheses!
}
myFunc <- outerFuncV2() # get a new function
ls(environment(myFunc)) # list all data attached to this function
myFunc()
```

```{block, type='rmd-details'}
Sometimes, in R, functions are called **closures** to emphasize that they are capturing variables from the parent environment in which they were created, to emphasize the data that they are bundled with. 
```



### Function Scope in Python

Python uses **lexical scoping** just like R! There's a famous acronym for the concept in Python: **LEGB**.

- L: Local, 
- E: Enclosing, 
- G: Global, and 
- B: Built-in.

A Python function will search for a variable in these [namespaces](https://docs.python.org/3/tutorial/classes.html#python-scopes-and-namespaces) in this order.^[Functions aren't the only thing that get their own namespace. For instance, [classes do as well](https://docs.python.org/3/tutorial/classes.html#a-first-look-at-classes). More information on classes is provided in Chapter \@ref(an-introduction-to-object-oriented-programming)]. 

"*Local*" refers to variables that are defined inside of the function's block. The function below uses the local `a` over the global one. 

```{python, collapse = TRUE}
a = 3
def silly_function():
  a = 22 # local a
  print("local variables are ", locals())
  return a + 20
silly_function()
silly_function.__code__.co_nlocals # number of local variables
silly_function.__code__.co_varnames # names of local variables
```

"*Enclosing*" refers to variables that were defined in the enclosing namespace, but not the global namespace. These variables are sometimes called **free variables.** In the example below, there is no local `a` variable for `inner_func`. But there is a global one and one in the enclosing namespace. It chooses the one in the enclosing namespace. 

```{python, collapse = TRUE}
a = "outside both"
def outer_func():
  a = "inside one"
  def inner_func():
    print(a)
  return inner_func
my_new_func = outer_func()
my_new_func()
my_new_func.__code__.co_freevars
```

"*Global*" scope contains variables defined in the module-level namespace. If the below example code was the entirety of your script, then `a` would be a global variable.

```{python, collapse = TRUE}
a = "outside both"
def outer_func():
  b = "inside one"
  def inner_func():
    print(a) 
  inner_func()
outer_func()
```


Just like in R, Python functions **cannot** necessarily find variables in an environment where the function was *called* in. For example, here is some code that mimics the above R example. Both `a` and `b` are accessible from within `inner_func`. That is due to LEGB.

```{python}
a = "outside both"
def outer_func():
  b = "inside one"
  def inner_func():
    print(a) 
    print(b)
  return inner_func() 
outer_func()
```

However, if we start using `outer_func` inside another function, *calling* it in another function, when it was *defined* somewhere else, well then it doesn't have access to some variables. You might be surprised at how the following code functions. Does this print the right string: `"this is the a I want to use now!"` No!

```{python}
def third_func():
  a = "this is the a I want to use now!"
  outer_func()
third_func() 
```

Again, these examples get at *functional programming*, which is discussed more in depth in chapter \@ref(functional-programming). There it will describe strategies to make your code easier to maintain (e.g. keep your functions "pure"!)


### Modifying a Function's Arguments

Can/should we modify a function's argument? The flexibility to do this sounds empowering; however, not doing it is recommended because it makes programs easier to reason about. 

#### Passing By Value In R

In R, it is *difficult* for a function to modify the variable that a user plugs in to a function as its argument.^[There are some exceptions to this, but it's generally true.] Consider the following code.

```{r, collapse=TRUE}
a <- 1
f <- function(arg){
  arg <- 2
  return(arg)
}
print(a)
print(f(a))
print(a)
```

The function `f` has an argument called `arg`. When `f(a)` is performed, changes are made to a *copy* of `a`. When a function constructs a copy of all input variables inside its body, this is called **pass-by-value** semantics. This copy is a temporary intermediate value that only serves as a starting point for the function to produce a return value of `2`.

`arg` could have been called `a`, and the same behavior will take place. However, giving these two things different names is helpful to remind you and others that R copies its arguments.

It is still possible to modify `a`, but I don't recommend doing this either. I will discuss this more in subsection \@ref(modifying-a-functions-arguments).


#### Passing By Assignment In Python

The story is more complicated in Python. Python functions have **pass-by-assignment** semantics. This is something that is very unique to Python. What this means is that your ability to modify the arguments of a function depends on 

- what the type of the argument is, and
- what you're trying to do to it. 

We will go throw some examples first, and then explain why this works the way it does. Here is some code that is analogous to the example above. 

```{python, collapse=TRUE}
a = 1
def f(arg):
  arg = 2
  return arg

print(type(a))
print(a)
print(f(a))
print(a)
```

In this case, `a` is not modified. That is because `a` is an `int`. `int`s are **immutable** in Python, which means that their [value](https://docs.python.org/3/reference/datamodel.html#objects-values-and-types) cannot be changed after they are created, either inside or outside of the function's scope. However, consider the case when `a` is a `list`, which is a **mutable** type. A mutable type is one that can have its value changed after its created. 

```{python, collapse=TRUE}
a = [999]
def f(arg):
  arg[0] = 2
  return arg

print(type(a))
print(a)
print(f(a))
print(a)
```

In this case `a` *is* modified. Changing the value of the argument *inside* the function effects changes to that variable outside of the function. 

Ready to be confused? What happens if we take in a list, but try to do something else with it. 

```{python, collapse=TRUE}
a = [999]
def f(arg):
  arg = [2]
  return arg

print(a)
print(f(a))
print(a)
```

That time `a` did not permanently change in the global scope. Why does this happen? I thought `list`s were mutable!

The reason behind all of this doesn't even have anything to do with functions, per se. Rather, it has to do with how Python manages, [objects, values, and types](https://docs.python.org/3/reference/datamodel.html#objects-values-and-types). It also has to do with what happens during [assignment](https://docs.python.org/3/reference/executionmodel.html#naming-and-binding).

Let's revisit the above code, but bring everything out of a function. Python is pass-by-assignment, so all we have to do is understand how assignment works. Starting with the immutable `int` example, we have the following.

```{python, collapse=TRUE}
# old code: 
# a = 1
# def f(arg):
#   arg = 2
#   return arg
a = 1    # still done in global scope
arg = a  # arg is a name that is bound to the object a refers to
arg = 2  # arg is a name that is bound to the object 2
print(arg is a)
print(id(a), id(arg))
print(a)
```

```{block, type='rmd-details'}
The [`id()`](https://docs.python.org/3/library/functions.html#id) function returns the **identity** of an object, which is kind of like its memory address. Identities of objects are unique and constant. If two variables, `a` and `b` say, have the same identity, `a is b` will evaluate to `True`. Otherwise, it will evaluate to `False`.
```

In the first line, the *name* `a` is bound to the *object* `1`. In the second line, the name `arg` is bound to the *object* that is referred to by the *name* `a`. After the second line finishes, `arg` and `a` are two names for the same object (a fact that you can confirm by inserting `arg is a` immediately after this line). 

In the third line, `arg` is bound to `2`. The variable `arg` can be changed, but only by re-binding it with a separate object. Re-binding `arg` does not change the value referred to by `a` because `a` still refers to `1`, an object separate from `2`. There is no reason to re-bind `a` because it wasn't mentioned at all in the third line. 

If we go back to the first function example, it's basically the same idea. The only difference, however, is that `arg` is in its own scope. Let's look at a simplified version of our second code chunk that uses a mutable list.

```{python, collapse=TRUE}
a = [999]
# old code:
# def f(arg):
#   arg[0] = 2
#   return arg
arg = a
arg[0] = 2
print(arg)
print(a)
print(arg is a)
```

In this example, when we run `arg = a`, the name `arg` is bound to the same object that is bound to `a`. This much is the same. The only difference here, though, is that because lists are mutable, changing the first element of `arg` is done "in place", and all variables can access the mutated object.

Why did the third example produce unexpected results? 

```{python, collapse=TRUE}
a = [999]
# old code
# def f(arg):
#   arg = [2]
#   return arg
arg = a
arg = [2]
print(arg is a)
print(a)
print(arg)
```

The difference is in the line `arg = [2]`. This rebinds the name `arg` to a different variable. `list`s are still mutable, but this has nothing to do with re-binding--re-binding a name works no matter what type of object you're binding it to. In this case we are re-binding `arg` to a completely different list.  



### Accessing and Modifying Non-Local Variables

In the last subsection, we were talking about variables that were passed in as arguments to a function. Here we are talking about variables that are not, but are still referred to inside a function's body. 

In general, even though it is possible to access and modify non-local variables in both languages, it is not a good idea. 

#### Accessing and Modifying Non-Local Variables in R

As Hadley Wickham writes in [his book](https://adv-r.hadley.nz/functions.html#dynamic-lookup), "[l]exical scoping determines where, but not when to look for values." R has **dynamic lookup**, meaning code inside a function will only try to access a referred-to variable when the function is *running*, not when it is defined.

Consider the R code below. 

```{r, collapse=TRUE}
# R
missileLaunchCodesSet <- TRUE
everythingIsSafe <- function(){
  return(!missileLaunchCodesSet)
}
missileLaunchCodesSet <- FALSE
# everythingIsSafe() # what happens if we call it?
```

`everythingIsSafe` is created in the global environment, and the global environment contains a Boolean variable called `missileLaunchCodesAreSet`. 

Now imagine sharing some code with a collaborator. Imagine, further, that your collaborator is the subject-matter expert, and knows little about R programming. Suppose that he changes a global variable in the script. Shouldn't this induce a relatively trivial change to the overall program? 

Let's explore this hypothetical further. Consider what could happen if any of the following (very typical) conditions are true:

- you or your collaborators aren't sure what `everythingIsSafe` will return because you don't understand dynamic lookup, or 
- it's difficult to visually keep track of all assignments to `missileLaunchCodesAreSet` (e.g. your script is quite long or it changes often), or
- you are not running code sequentially (e.g. you are testing chunks at a time instead of clearing out your memory and `source()`ing from scratch, over and over again).

In each of these situations, understanding of the program would be compromised. However, if you follow the above principle of never referring to non-local variables in function code, all members of the group could do their own work separately, minimizing the dependence on one another. 

Another reason violating this could be troublesome is if you define a function that refers to a nonexistent variable. *Defining* the function will never throw an error because R will assume that variable is defined in the global environment. *Calling* the function might throw an error, unless you accidentally defined the variable, or if you forgot to delete a variable whose name you no longer want to use. 

```{r, collapse = TRUE}
# R
myFunc <- function(){
  return(varigbleNameWithTypo)
}
```
Running the above code to define `myFunc` will not throw an error, even if you think it should!

#### Accessing and Modifying Non-Local Variables in Python

It is the same exact situation in Python. Consider `everything_is_safe`, a function that is analogous to `everythingIsSafe`.

```{python, collapse=TRUE}
# python
missile_launch_codes_set = True
def everything_is_safe():
  return not missile_launch_codes_set

missile_launch_codes_set = False
everything_is_safe()
```

We can also define `my_func`, which is analogous to `myFunc`. Defining this function doesn't throw an error either!

```{python, collapse = TRUE}
# python
def my_func():
  return varigble_name_with_typo
```

So stay away from referring to variables outside the body of your function! 


#### Modifying Non-Local Variables In R

Now what if we want to be extra bad, and in addition to *accessing* global variables, we *modify* them, too.

```{r, collapse=TRUE}
a <- 1
f <- function(arg){
  arg <- 2
  a <<- arg
  # return(arg) # no return value
}
print(a)
print(f(a))
print(a)
```

In the program above, `arg` creates a copy of `a`. It assigns `2` to that copy. Then it takes that `2` and writes it to the global variable in the parent environment. Notice that the function can take in different inputs, but the global assignment is hard-coded.



#### Modifying Non-Local Variables In Python


Finally, there is something in Python that is like R's super assignment operator (`<<-`). It is the `global` keyword. This will let you *modify* global variables. 

```{block, type='rmd-details'}
Referring to global variables *without* modifying them was always allowed, even without using the `global` keyword. This keyword should be used sparingly, and when it is used, it identifies that a function causes **side effects**, which are changes in some variable defined outside of the function's scope.
```

```{python, collapse = TRUE}
a = 1
def increment_a():
  global a
  a += 1
increment_a()
increment_a()
increment_a()
print(a)
```


Here's a last example that will be important for us in particular. Notice that Numpy `array`s are mutable.

```{python, collapse = TRUE}
import numpy as np
my_array = np.array([1,2,3])
def make_calc(arr):
  arr[0] = np.average(my_array)
  return 2*arr
result = make_calc(my_array)
print(result)
print(my_array) # watch out: side effect
```


## Categorical Data 


### Categorical Data in R

Categorical data is typically stored in a [`factor`](https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Factors) variable in R. For example, say we asked three people what their favorite season was. The data might look something like this.

```{r, collapse = TRUE}
responses <- factor(c("autumn", "summer", "summer"), 
                    levels = c("autumn", "summer", "spring", "winter"))
levels(responses)
contrasts(responses)
is.factor(responses)
is.ordered(responses)
```

`factor`s have a `levels` attribute, which is comprised of all the possible values that each response could be. They also have a `contrasts` attribute, which will be important once you start using `factor`s as inputs to functions such as `lm`. In the case of using `factor`s as inputs to `lm()`, the `factor` would tell `lm()` *how* to create the dummy predictors in a linear regression model. It's perfectly fine if you're rusty on regression--the reason I mention this is that in Python, dummy variable construction is done more explicitly/manually.

```{block, type='rmd-caution'}
In the above example, there wasn't at least one person who prefers each season (that's a confusing sentence). Here, if we did not specify a `levels` argument, there would only be two levels. This is a common source of bugs! Another source of bugs: what if some people say "autumn" and others say "fall"?
```

`factor`s can be ordered or unordered. Ordered `factor`s are for ordinal data. As another example, say we asked ten people how much they liked programming, and they could only respond "love it", "hate it", or "it's okay". The data might look something like this.

```{r, collapse = TRUE}
responses <- factor(c("love it", "it's okay", "love it", 
                      "love it", "it's okay", "love it", 
                      "love it", "love it", "it's okay", 
                      "it's okay"), 
                    levels = c("hate it", "it's okay", "love it"),
                    ordered = TRUE)
levels(responses)
contrasts(responses)
is.factor(responses)
is.ordered(responses)
```

Whether a `factor` is ordered or not can affect its `contrasts` and the behavior of functions it is fed into. Intuitively, it should be clear when to impose ordering or not. In the first example, there isn't a clear ordering of the seasons (which one should come first?). In the second example, we are looking at responses to a "how much" question. 

Here's a third example. We can take non-categorical data, and `cut` it into something categorical. 

```{r, collapse = TRUE}
stockReturns <- rnorm(10) # not categorical here
typeOfDay <- cut(stockReturns, breaks = c(-Inf, 0, Inf)) 
typeOfDay
levels(typeOfDay)
is.factor(typeOfDay)
is.ordered(typeOfDay)
```

### Categorical Data in Python

[Categorical data can be handled with the Pandas' library](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html), which takes a lot of inspiration from R. We've talked about `Series` objects before in section \@ref(vectorization-in-python), and here we will use them again. All we have to do to make a `Series` object categorical is to change its `dtype`. The `dtype` we provide will control the categories (like `levels` in R), and whether it's ordered or not.


```{python, collapse = TRUE}
import pandas as pd
from pandas.api.types import CategoricalDtype

cat_type = CategoricalDtype(categories=["autumn", "summer", "spring", "winter"], 
                            ordered=False)
responses = pd.Series(["autumn", "summer", "summer"], 
                      dtype = cat_type)
responses
responses.cat.categories
responses.cat.ordered
```

Pandas also provides a `pd.cut()` function, which can return either of these types, or even a regular Numpy array.

```{python, collapse = TRUE}
stock_returns = np.random.normal(size=10) # not categorical here
type_of_day = pd.cut(stock_returns, [-np.inf, 0, np.inf], labels = ['bad day', 'good day']) 
type_of_day
type(type_of_day)
type_of_day = pd.Series(type_of_day)
type(type_of_day)
type_of_day.cat.categories
type_of_day.cat.ordered
```


```{block, type='rmd-details'}
You'll notice that, in this instance, `pd.cut` did not return a `Series` object. It can, but `pd.cut`'s return type will depend on the inputs you feed in. In this case, it returned a [`Categorical`](http://pandas-docs.github.io/pandas-docs-travis/reference/api/pandas.Categorical.html#pandas.Categorical), which is not the same thing as a `Series`. In the code above, I had to convert it back before accessing the `cat` attribute.
```


## Data Frames

As data scientists, most of the time, our data set will be stored as a data frame.

### Data Frames in R

Let's consider as an example Fisher's "Iris" data set. We will read this data set in from a comma separated file (more on input/output in chapter \@ref(input-and-output)). This file can be downloaded from this link: [https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris). 


```{r, collapse=TRUE}
irisData <- read.csv("data/iris.csv", header = F)
head(irisData)
typeof(irisData)
class(irisData) # we'll talk more about classes later
dim(irisData)
nrow(irisData)
ncol(irisData)
```

```{block, type='rmd-caution'}
Do not rely on the default arguments of `read.csv` or `read.table`! After you read in a data frame, always check the first few rows to make sure that 

1. The number of columns is correct because the correct column *separator* was used (c.f. `sep=`),
2. column names were parsed correctly, if there were some in the raw text file,
3. the first row of data wasn't used as a column name sequence, if there weren't column names in the text file, and
4. the last few rows aren't reading in empty spaces
5. character columns are read in correctly (c.f. `stringsAsFactors=`), and
6. special characters signifying missing data were correctly identified (c.f. `na.strings=`).
```


There are some exceptions, but most data sets can be stored as a `data.frame`. This is because usually a data set comes in a two-dimensional shape Looking at one particular row gives you an observation with all its variables. Looking at an particular column gives you one particular variable for each observation. 

[A `data.frame` is a special case of a `list`](https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Data-frame-objects). Every element of the list is a column. Columns can be `vector`s or `factor`s, and they can all be of a different type. This is one of the biggest differences between data frames and `matrix`s. They are both two-dimensional, but a `matrix` needs elements to be all the same type. Unlike a general `list`, a `data.frame` requires all of its columns to have the same number of elements. In other words, the `data.frame` is not a "*ragged*" list.

Often times you will need to extract pieces of information from a `data.frame`. This can be done in many ways. If the columns have names, you can use the `$` operator to access a single column. Accessing a single column might be followed up by creating a new vector. You can also use the `[` operator to access multiple columns by name.  

```{r, collapse=TRUE}
colnames(irisData) <- c("sepal.length", "sepal.width", "petal.length","petal.width", "species")
firstCol <- irisData$sepal.length
head(firstCol)
firstTwoCols <- irisData[c("sepal.length", "sepal.width")] 
head(firstTwoCols)
```

The `[` operator is also useful for selecting rows and columns by index numbers, or by some logical criteria.

```{r, collapse=TRUE}
topLeft <- irisData[1,1] # first row, first col
topLeft
firstThreeRows <- irisData[1:3,] # rows 1-3, all cols
firstThreeRows
setosaOnly <- irisData[irisData$species == "Iris-setosa",] # rows where species column is setosa
head(setosaOnly)
```

In the code above, `irisData$species == "Iris-setosa"` creates a logical vector (try it!) using the vectorized `==` operator. The `[` operator selects the rows for which the corresponding element of this logical vector is `TRUE`. 

```{block, type='rmd-details'}
Be careful: depending on how you use the square brackets, you can either get a `data.frame` or a `vector.` As an example, try both `class(irisData[,1])` and `class(irisData[,c(1,2)])`. 
```


In R, `data.frame`s have row names. You can get/set this character `vector` with the `rownames()` function. You can access rows by name using the square bracket operator. Personally, I don't typically use this functionality that often.

```{r, collapse = TRUE}
head(rownames(irisData))
rownames(irisData) <- as.numeric(rownames(irisData)) + 1000
head(rownames(irisData))
irisData["1002",]
```

### Data Frames in Python


The Pandas library in Python has data frames that are modeled after R's. 

```{python, collapse=TRUE}
import pandas as pd
iris_data = pd.read_csv("data/iris.csv", header = None)
iris_data.head()
iris_data.shape
len(iris_data) # num rows
len(iris_data.columns) # num columns
iris_data.dtypes
```

The structure is very similar to that of R's data frame. It's two dimensional, and you can [access columns and rows by name or number.](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html) Each column is a `Series` object, and each column can have a different `dtype`, which is analogous to R's situation. Again, because the elements need to be the same type along columns only, this is a big difference between 2-d Numpy `array`s and `DataFrame`s.


Just like in R, you can access columns by name. You do that using square brackets. Observe how similar this code is to the corresponding R code above.

```{python, collapse=TRUE}
iris_data.columns = ["sepal.length", "sepal.width", "petal.length",
                     "petal.width", "species"]
first_col = iris_data['sepal.length']
first_col.head()
first_two_cols = iris_data[["sepal.length", "sepal.width"]]
first_two_cols.head()
```


```{block, type='rmd-details'}
Notice that `iris_data['sepal.length']` returns a `Series` and `iris_data[["sepal.length", "sepal.width"]` returns a Panda's `DataFrame`. This behavior is similar to what happened in R's. For more details, click [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#indexing-selection).
```

You can select columns and rows by number with the [`.iloc` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html). `iloc` is (probably) short for "integer location." 

```{python, collapse=TRUE}
# specify rows/cols by number
top_left = iris_data.iloc[0,0]
top_left
first_three_rows = iris_data.iloc[:3,]
first_three_rows
#setosa_only = iris_data[irisData$species == "Iris-setosa",]  # easieriwith loc?
#head(setosaOnly)
```

Selecting columns by anything besides integer number can be done with the [`.loc()` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html). You should generally prefer this method to access columns because accessing things by *name* instead of *number* is more readable. Here are some examples. 

```{python, collapse=TRUE}
sepal_w_to_pedal_w = iris_data.loc['sepal.width':'pedal.width']
sepal_w_to_pedal_w
setosa_only = iris_data.loc[iris_data['species'] == "Iris-setosa",]
setosa_only.head()
```

Notice we used a `slice` to access many columns by only referring to the left-most and the right-most. This does not work with the regular square bracket operator. The second example filters out the rows where the `"species"` column elements are equal to `"Iris-setosa"`.

Each `DataFrame` in Pandas comes with an `.index` attribute. This is analogous to a row name in R, but it's much more flexible because the index can take on a variety of types. This can help us highlight the difference between `.loc` and `.iloc`. Recall that `.loc` was label-based selection. Labels don't necessarily have to be strings. Consider the following example

```{python, collapse = TRUE}
iris_data.index
iris_data = iris_data.set_index(iris_data.index[::-1]) # reverse the index
iris_data.head(2)
iris_data.tail(2)
iris_data.loc[0]
iris_data.iloc[0] 
```

`iris_data.loc[0]` selects the `0`th index. The second line reversed the indexes, so this is actually the last row. If you want the first row, use `iris_data.iloc[0]`.

<!-- [`DataFrame`s have a huge amount  many useful attributes and methods, too.](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#) -->

### Row Names and Indexes

In Python, Pandas `DataFrame`s have an 

```{python, collapse = TRUE}
iris_data.index

```




### Getting Versus Setting




## Exercises

All answers to questions related to R should be written in a file named `data_types_exercises.R`. All answers to questions related to Python should be written in a file named `data_types_exercises.py`.


1. Which Python type is appropriate for each piece of data?

  a. TODO
  b. TODO
  
  
2. In R, say you have a vector of prices of some financial asset:

```{R, collapse = TRUE}
prices <- c(100.10, 95.98, 100.01, 99.87)
```

a.
Convert this vector into a vector of *log returns*. Call the variable `log_returns`. If $p_t$ is the price at time $t$, the log return ending at time $t$ is 
$$ r_t = \log \left( \frac{p_t}{p_{t-1}} \right) = \log p_t - \log p_{t-1}$$

b.
Do the same for *arithmetic returns*. These are regular percent changes if you scale by $100$. Call the variable `arith_returns`. The mathematical formula you need is

$$ a_t = \left( \frac{p_t - p_{t-1} }{p_{t-1}} \right) \times 100 $$

3. Assume we are interested in the probability that a normal random variable with mean $5$ and standard deviation $6$ is greater than $6$.

We will make use of the *Monte Carlo* [@monte-carlo-stat-methods] method below. It is a technique to approximate expectations and probabilities. If $n$ is a large number, then the right hand side of 
$$
\mathbb{P}(X > 6) \approx \frac{1}{n}\sum_{i=1}^n \mathbf{1}(X_i > 6)
$$
is an accurate approximation. If you haven't seen an **indicator** function before, it is defined as 

$$
\mathbf{1}(X_i > 6)
=
\begin{cases}
1 & X_i > 6 \\
0 & X_i \le 6
\end{cases}.
$$

a. Evaluate this probability exactly in R and assign it to the variable `exactExceedanceProb` 

b. Evaluate this probability exactly in Python and assign it to the variable `exact_exceedance_prob`

c. In R, use the Monte Carlo method to estimate the probability. Use one thousand samples. Assign it to the variable `approxExceedanceProb`

d. In Python, use the Monte Carlo method to estimate the probability. Use one thousand samples. Assign it to the variable `approx_exceedance_prob`


4. For a collection of random variables $X_1, \ldots, X_n$, a *covariance matrix* arranges all of the covariances between every possible pair of random variables:

$$
\begin{bmatrix}
\text{Cov}(X_1, X_1) & \text{Cov}(X_1, X_2) & \cdots & \text{Cov}(X_1, X_n) \\
\text{Cov}(X_2, X_1) & \text{Cov}(X_2, X_2) & \cdots & \text{Cov}(X_2, X_n) \\
\vdots & \vdots & \ddots & \vdots\\
\text{Cov}(X_n, X_1) & \text{Cov}(X_n, X_2) & \cdots & \text{Cov}(X_n, X_n) \\
\end{bmatrix}
$$
where 
$$\text{Cov}(X_i, X_j) = \mathbb{E}\left[(X_i - \mathbb{E}[X_i])((X_j - \mathbb{E}[X_j])\right]$$
 is the covariance between $X_i$ and $X_j$ resting in row $i$ and column $j$. 
 
 Using this definition, it is easy to show that $\text{Cov}(X_i, X_i) = \mathbb{E}\left[(X_i - \mathbb{E}[X_i])^2\right] = \text{Var}(X_i)$.

An **exchangeable** covariance matrix for a random vector is one that has all the same variances, and all the same covariances. In other words, it has two unique elements: the diagonal elements should be the same, and the off-diagonals should be the same. 

a. In R, generate $10$ $4 \times 4$ exchangeable covariance matrices, each with $2$ as the variance, and have the possible covariances take values in the collection $0,.01,.02, ..., .09.$  Store these $10$ covariance matrices in a three-dimensional array. The first index should be each matrix's row index, the second should be the column index of each matrix, and the third index should be the "layer" or "slice" indicating which of the $10$ matrices you have. Name this array `myCovMats`

b. Do the same thing in Python, but call the variable `my_cov_mats`

5. In R, read in the `cars.csv` data set using `read.table()` (more on IO in chapter TODO). Find the average `EngineSize`, `Cylinders`, `Horsepower`, `MPG_City`, `MPG_Highway`, `Weight`, `Wheelbase` and `Length` **for each type of vehicle** (i.e. `Hybrid` `Sedan` `Sports`, `SUV`, `Truck` and `Wagon`). Which of these averages is an `NA`? How many observations in that column are missing? 

6. In Python (TODO finish this question), read in the `cars.csv` data set using `read.table()` (more on IO in chapter TODO). Find the average `EngineSize`, `Cylinders`, `Horsepower`, `MPG_City`, `MPG_Highway`, `Weight`, `Wheelbase` and `Length` **for each type of vehicle** (i.e. `Hybrid` `Sedan` `Sports`, `SUV`, `Truck` and `Wagon`). Which of these averages is an `NA`? How many observations in that column are missing? 


6.  Here are two lists in R:

```{R, collapse = TRUE}
l1 <- list(first="a", second=1)
l2 <- list(first=c(1,2,3), second = "statistics")
```



a. Make a new `list` that is these two lists above "squished together." It has to be length $4$, and each element is one of the elements of $l1$ and $l2$. Call this list `l3`.

b. Delete all the "tags" or "names" of these four elements.

c. Make a `vector` of all the unique single digit numbers in both of the lists. You should end up with the vector with elements `1`, `2`, and `3`.


7.  Here are two `dict`s in Python:

```{python, collapse = TRUE}
d1 = { "first" : "a", "second" : 1}
d2 = { "first" : [1,2,3], "second" : "statistics"}
```



a. Make a new `list` that is these two `dict`s above "squished together" (why can't it be another `dict`?) It has to be length $4$, and each value is one of the values of $d1$ and $d2$. Call this list `l3`.


8. How might you explain the difference between Python and R's type systems? What do you know about the historical development of these languages that might assist your explanation?


9. Example on underflow and overflow.


12. infix functions in R

13. setter methods in R

14. Python functions that have return types specified

15. log-sum-exp trick problem

16. linear algebra problems

17. demo complete.cases versus dropna

18. question on likert scales

19. question with cut that induces NAs

